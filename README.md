# Regression with Perceptron 

This is fifth assignment of Introduction to Machine Learning (COMP 462) course.In this assignment, we used three diﬀerent dataset,
one of which was given, two of which were generated by us. The datasets given in Figure 1. Second dataset
generated by adding little noise to *x^3 + 150* function when x values are between
-3 and 10. Third dataset generated by adding little noise to *6 ∗ x + 10* function
when x values are between -3 and 10.

![image](https://user-images.githubusercontent.com/45906647/56856177-c20cbb00-695d-11e9-9092-9fe4840f1d8b.png)


## Stochastic Gradient Descent
In gradient descent, a batch is the total number of examples we use to calculate
the gradient in one iteration. We assumed all example in dataset as one batch on
standard gradient descent algorithm. When working on dataset contains billions
data example, a large batch may cause long times to compute weights. Therefore,
we use a stochastic gradient descent algorithm which update weights for each
sample in dataset. The algorithm steps given as following.

Each training example is a pair of the form *x*, *t* where *x* is the vector
of input values, and *t* is the target output value. *η* is the learning rate

1. Initialize each *w<sub> i</sub>* to some small random value
2. Until the termination condition is met, Do
  * For each *x*, *t* in the training set, Do
    * Input the instance *x* to the unit and compute the output o
    * For each linear unit weight *w<sub> i</sub>* , Do
      * *w<sub> i</sub>* ← *w<sub> i</sub>* + η(t - o)x<sub>i</sub>
      

## Regression Results

Results for given dataset given in Figure 2. Perceptron easily minimize the loss
error and ﬁt the datas. Figure 2.d show that perceptron ﬁt the data better between 0.007 and 0.012 values of learning rate. For greater values of learning rates,
error goes to inﬁnity.


![image](https://user-images.githubusercontent.com/45906647/56856287-fa14fd80-695f-11e9-9deb-69bf42cbf4cc.png)

Results for second dataset which generated by us given in Figure 3. Second dataset generated by adding little noise to *x^3 + 150* function when x values are between -3 and 10. Because of our generated data are not linear, the perceptron
could not ﬁt the data successfully. Figure 3.d show that loss increase after 0.06
values of leaning rate.


![image](https://user-images.githubusercontent.com/45906647/56856292-2761ab80-6960-11e9-94eb-02f5321e320a.png)

Results for third dataset which generated by us given in Figure 4. Third dataset generated by adding little noise to *6 ∗ x + 10* function when x values are between -3 and 10. Because of our generated data are crated from linear function, the perceptron ﬁt the data successfully. Figure 4.d show that perceptron ﬁt the data better between 0.001 and 0.04 values of learning rate. For greater
values of learning rates perceptron could not ﬁt the data and converges to inﬁnity.



![image](https://user-images.githubusercontent.com/45906647/56856344-65130400-6961-11e9-9344-3c07690c24e0.png)
